# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from tqdm import tqdm  # Import tqdm for progress bar

# Free MPS memory before running
if torch.backends.mps.is_available():
    torch.mps.empty_cache()
    torch.mps.synchronize()

# Load tokenizer and model
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Check if MPS (Apple GPU) is available, otherwise use CPU
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# Load model onto the selected device
model = AutoModelForCausalLM.from_pretrained(model_name)
model.to(device)

# Set pad token ID (to avoid warnings)
tokenizer.pad_token = tokenizer.eos_token


while True:
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    user_input = input("\në¬¸ì¥ì„ dd ì…ë ¥í•˜ì„¸ìš”: ")

    # ì…ë ¥ ë¬¸ì¥ í† í°í™”
    inputs = tokenizer(user_input, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs.input_ids.to(device)
    attention_mask = inputs.attention_mask.to(device)

    # Maximum length of generated text
    max_length = 100  # ì†ë„ì™€ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 100ìœ¼ë¡œ ì„¤ì •

    # Create a progress bar
    print("\nGenerating text...")
    generated_ids = input_ids  # ì´ˆê¸° ì…ë ¥ê°’ ì„¤ì •
    progress_bar = tqdm(total=max_length, desc="Generating", unit="token")

    # Step-by-step token generation
    for _ in range(max_length):
        output = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=100,  # ğŸš€ max_new_tokens ëŒ€ì‹  ì‚¬ìš©
            temperature=0.7,
            top_k=50,
            top_p=0.9,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=True,
        )

        # ìƒˆ í† í° ì¶”ê°€
        new_token_id = output[0, -1].unsqueeze(0).unsqueeze(0)  # ë§ˆì§€ë§‰ ìƒì„±ëœ í† í° ì¶”ì¶œ
        generated_ids = torch.cat([generated_ids, new_token_id], dim=-1)  # ìƒˆë¡œìš´ í† í° ì¶”ê°€

        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
        progress_bar.update(1)

        # ë§Œì•½ ëª¨ë¸ì´ EOS(ì¢…ë£Œ) í† í°ì„ ìƒì„±í•˜ë©´ ì¢…ë£Œ
        if new_token_id.item() == tokenizer.eos_token_id:
            break

    progress_bar.close()  # ì§„í–‰ ì™„ë£Œ í›„ tqdm ì¢…ë£Œ

    # ìƒì„±ëœ ë¬¸ì¥ ë””ì½”ë”©
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ìƒì„±ëœ í…ìŠ¤íŠ¸ ===")
    print(generated_text)
