from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList
import torch
import sys
import time


class StreamCriteria(StoppingCriteria):
    """ì‹¤ì‹œê°„ ë‹¨ì–´ ì¶œë ¥ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        last_token = input_ids[0, -1].item()
        last_word = self.tokenizer.decode([last_token], skip_special_tokens=True)
        sys.stdout.write(last_word)
        sys.stdout.flush()
        time.sleep(0.05)  # ìì—°ìŠ¤ëŸ¬ìš´ ì¶œë ¥ ì†ë„ ì¡°ì ˆ
        return False  # Falseë¥¼ ë°˜í™˜í•˜ë©´ ê³„ì† ìƒì„±


# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# âœ… CUDA ì‚¬ìš© ì—¬ë¶€ í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¹ Using device: {device}")

# âœ… ëª¨ë¸ ë¡œë“œ (FP16 ì‚¬ìš©)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16
).to(device)

print("\nğŸ’¬ [ëŒ€í™” ì‹œì‘] ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: Ctrl+C)\n")

try:
    while True:
        user_input = input("ğŸ‘¤ ì‚¬ìš©ì: ")

        if not user_input.strip():  # ë¹ˆ ì…ë ¥ ë°©ì§€
            print("âš ï¸ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”!")
            continue

        inputs = tokenizer(user_input, return_tensors="pt").to(device)

        print("\nğŸ¤– ëª¨ë¸:", end=" ", flush=True)

        stopping_criteria = StoppingCriteriaList([StreamCriteria(tokenizer)])

        model.generate(
            **inputs,
            max_length=500,
            temperature=0.7,
            do_sample=True,
            stopping_criteria=stopping_criteria
        )

        print("\n")  # ì¤„ë°”ê¿ˆ ì¶”ê°€

except KeyboardInterrupt:
    print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
