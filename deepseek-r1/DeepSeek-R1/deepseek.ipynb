{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ff0c9c-4de2-459c-b437-c17030092f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a494188-734f-4d0a-8d54-562d1f674eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129bd3bafec448ae875c5368dfc73ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b30f4-f383-4b5a-abe5-30485774ca4c",
   "metadata": {},
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Free MPS memory before running\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    torch.mps.synchronize()\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check if MPS (Apple GPU) is available, otherwise use CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model onto the selected device\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Set pad token ID (to avoid warnings)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Custom stopping criteria (to limit token count manually)\n",
    "class MaxTokenStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_new_tokens):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.current_tokens = 0\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        self.current_tokens += 1\n",
    "        return self.current_tokens >= self.max_new_tokens\n",
    "\n",
    "\n",
    "while True:\n",
    "    # 사용자 입력 받기\n",
    "    user_input = input(\"\\n문장을 입력하세요: \")\n",
    "\n",
    "    # 입력 문장 토큰화 (Set attention_mask to avoid unexpected behavior)\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Maximum length of generated text\n",
    "    max_length = 100  # Reduce length for better performance\n",
    "\n",
    "    # Create a progress bar\n",
    "    print(\"\\nGenerating text...\")\n",
    "    with tqdm(total=max_length, desc=\"Generating\", unit=\"token\") as pbar:\n",
    "        stopping_criteria = StoppingCriteriaList([MaxTokenStoppingCriteria(max_length)])\n",
    "\n",
    "        # Step-by-step token generation\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_length,  # Only generate `max_length` tokens\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,  # Enable sampling for more diverse output\n",
    "            stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        # Update tqdm progress bar dynamically\n",
    "        generated_tokens = output.shape[1] - input_ids.shape[1]\n",
    "        pbar.update(generated_tokens)\n",
    "\n",
    "    # 생성된 문장 디코딩\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"\\n=== 생성된 텍스트 ===\")\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b9c60-e3d3-4058-8ae2-199b19fd84ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89596e13-8a93-463d-80ba-5f195715ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "문장을 입력하세요:  안녕\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating: 100%|██████████████████████████| 100/100 [00:18<00:00,  5.37token/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 생성된 텍스트 ===\n",
      "안녕, machine learning beginner. Want to learn from scratch the deep learning part.\n",
      "\n",
      "I have a question: what is the best way to start with neural networks?\n",
      "\n",
      "My initial thought was that I should just use Keras or TensorFlow and write some code.\n",
      "\n",
      "But when I tried it before, things didn't go well; my models were overfitting on training data.\n",
      "\n",
      "So perhaps another approach would be better.\n",
      "\n",
      "Wait, maybe using PyTorch? It's an alternative framework for building models quickly and having control\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "문장을 입력하세요:  한국말로 대답해줘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████████████████████| 100/100 [00:05<00:00, 17.48token/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 생성된 텍스트 ===\n",
      "한국말로 대답해줘! 2017년 5월 3일 라는 기사의 Readers Response를 주세요. (가이mtplexon)\n",
      "<pre><code>\n",
      "</code></pre>\n",
      "\n",
      "```\n",
      "def get_message():\n",
      "    return 'The answer is \"This is the correct response to your question.\"'\n",
      "\n",
      "message = get_message()\n",
      "print(message)\n",
      "\n",
      "Wait, but you need to make this function work in a way that when called with any arguments, it returns an empty string.\n",
      "\n",
      "So\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tokens \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_new_tokens\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# 사용자 입력 받기\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m문장을 입력하세요: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# 입력 문장 토큰화 (Set attention_mask to avoid unexpected behavior)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(user_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepseek/lib/python3.8/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepseek/lib/python3.8/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Free MPS memory before running\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    torch.mps.synchronize()\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check if MPS (Apple GPU) is available, otherwise use CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model onto the selected device\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Set pad token ID (to avoid warnings)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Custom stopping criteria (to limit token count manually)\n",
    "class MaxTokenStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_new_tokens):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.current_tokens = 0\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        self.current_tokens += 1\n",
    "        return self.current_tokens >= self.max_new_tokens\n",
    "\n",
    "\n",
    "while True:\n",
    "    # 사용자 입력 받기\n",
    "    user_input = input(\"\\n문장을 입력하세요: \")\n",
    "\n",
    "    # 입력 문장 토큰화 (Set attention_mask to avoid unexpected behavior)\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Maximum length of generated text\n",
    "    max_length = 100  # Reduce length for better performance\n",
    "\n",
    "    # Create a progress bar\n",
    "    print(\"\\nGenerating text...\")\n",
    "    with tqdm(total=max_length, desc=\"Generating\", unit=\"token\") as pbar:\n",
    "        stopping_criteria = StoppingCriteriaList([MaxTokenStoppingCriteria(max_length)])\n",
    "\n",
    "        # Step-by-step token generation\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_length,  # Only generate `max_length` tokens\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,  # Enable sampling for more diverse output\n",
    "            stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        # Update tqdm progress bar dynamically\n",
    "        generated_tokens = output.shape[1] - input_ids.shape[1]\n",
    "        pbar.update(generated_tokens)\n",
    "\n",
    "    # 생성된 문장 디코딩\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"\\n=== 생성된 텍스트 ===\")\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9078c-0437-4b2e-b8ff-aaf81054fa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
